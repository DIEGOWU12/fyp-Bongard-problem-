import requests
from bs4 import BeautifulSoup
import time
import random
import os
import csv
from urllib.parse import urljoin
from concurrent.futures import ThreadPoolExecutor
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# ====================================================================
# 1. è®¾ç½®ç›®æ ‡å’Œå‚æ•°
# ====================================================================
BASE_URL = "https://oebp.org/BP"
START_ID = 1  
END_ID = 3000  
OUTPUT_DIR = "Bongard_Dataset_v2"
SOLUTION_FILE = os.path.join(OUTPUT_DIR, "solutions_and_images.csv")
MAX_WORKERS = 7  # å¹¶å‘çº¿ç¨‹æ•°ï¼Œå»ºè®® 5-10ï¼Œä¸è¦å¤ªé«˜ä»¥å…è¢«å°

# åˆå§‹åŒ– Session æé«˜è¿æ¥æ•ˆç‡
session = requests.Session()
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}
retry_strategy = Retry(
    total=3,                          # æœ€å¤§é‡è¯•æ¬¡æ•°
    backoff_factor=1,                 # é—´éš”æ—¶é—´ç³»æ•° (1s, 2s, 4s...)
    status_forcelist=[429, 500, 502, 503, 504], # é‡åˆ°è¿™äº›çŠ¶æ€ç æ‰é‡è¯•
)

adapter = HTTPAdapter(max_retries=retry_strategy)
session.mount("https://", adapter)
session.mount("http://", adapter)
session.headers.update(HEADERS)

# åˆ›å»ºè¾“å‡ºç›®å½•
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

# ====================================================================
# 2. è¾…åŠ©å‡½æ•°
# ====================================================================

def download_and_save_image(img_url, filename, bp_id):
    """ä¸‹è½½å›¾ç‰‡"""
    bp_dir = os.path.join(OUTPUT_DIR, f"BP{bp_id}")
    if not os.path.exists(bp_dir):
        os.makedirs(bp_dir)
        
    image_path = os.path.join(bp_dir, filename)
    if os.path.exists(image_path):
        return os.path.join(f"BP{bp_id}", filename)

    try:
        img_response = session.get(img_url, timeout=10)
        if img_response.status_code == 200:
            with open(image_path, 'wb') as f:
                f.write(img_response.content)
            return os.path.join(f"BP{bp_id}", filename)
    except Exception:
        pass
    return "ä¸‹è½½å¤±è´¥"

def save_solution_to_txt(bp_id, solution_text):
    """ä¿å­˜æ–‡æœ¬"""
    bp_dir = os.path.join(OUTPUT_DIR, f"BP{bp_id}")
    if not os.path.exists(bp_dir):
        os.makedirs(bp_dir)
        
    txt_path = os.path.join(bp_dir, "solution.txt")
    try:
        with open(txt_path, 'w', encoding='utf-8') as f:
            f.write(solution_text)
        return os.path.join(f"BP{bp_id}", "solution.txt")
    except Exception:
        return "å†™å…¥å¤±è´¥"

# ====================================================================
# 3. æ ¸å¿ƒæŠ“å–é€»è¾‘
# ====================================================================

def fetch_bongard_problem(bp_id):
    """çˆ¬å–å•ä¸ªé—®é¢˜çš„æ ¸å¿ƒé€»è¾‘"""
    url = f"{BASE_URL}{bp_id}"
    try:
        response = session.get(url, timeout=10)
        if response.status_code != 200:
            return None

        soup = BeautifulSoup(response.text, 'html.parser')

        # --- æ­¥éª¤ 1: å…ˆæ‰¾å›¾ç‰‡å¹¶æ ¡éªŒæ•°é‡ (æ ¸å¿ƒä¿®å¤ç‚¹) ---
        img_tags = soup.find_all('img', src=lambda src: src and '/examples/' in src)
        if len(img_tags) != 12:
            # å›¾ç‰‡ä¸è¶³ 12 å¼ ï¼Œç›´æ¥é€€å‡ºï¼Œä¸åˆ›å»ºæ–‡ä»¶å¤¹
            return None

        # --- æ­¥éª¤ 2: æ ¡éªŒé€šè¿‡ï¼Œæå–æ–‡æœ¬ ---
        solution_text = "æœªæ‰¾åˆ°è§£å†³æ–¹æ¡ˆæ–‡æœ¬"
        bp_link_tag = soup.find('a', href=f'/BP{bp_id}', string=f'BP{bp_id}')
        if bp_link_tag:
            tr = bp_link_tag.find_parent('tr')
            if tr:
                tds = tr.find_all('td')
                if len(tds) >= 3:
                    solution_text = tds[2].get_text(strip=True)

        # --- æ­¥éª¤ 3: å¼€å§‹å†™å…¥ç£ç›˜ ---
        txt_path = save_solution_to_txt(bp_id, solution_text)
        
        image_paths = []
        for img_tag in img_tags:
            img_src = img_tag['src']
            img_url = urljoin("https://oebp.org", img_src)
            filename = os.path.basename(img_src)
            path = download_and_save_image(img_url, filename, bp_id)
            image_paths.append(path)

        # æ¨¡æ‹Ÿä¸€ç‚¹ç‚¹å»¶è¿Ÿï¼Œé˜²æ­¢ç»™æœåŠ¡å™¨å¤ªå¤§å‹åŠ›
        time.sleep(random.uniform(1, 2))

        print(f"âœ… BP{bp_id} å¤„ç†æˆåŠŸ")
        return {
            'BP_ID': f"BP{bp_id}",
            'solution': solution_text,
            'solution_txt_path': txt_path,
            'image_paths': image_paths
        }

    except Exception as e:
        print(f"âŒ BP{bp_id} é”™è¯¯: {e}")
        return None

# ====================================================================
# 4. æ‰§è¡Œå¤šçº¿ç¨‹ä»»åŠ¡
# ====================================================================

if __name__ == "__main__":
    print(f"ğŸš€ å¼€å§‹çˆ¬å–ä»»åŠ¡ï¼Œçº¿ç¨‹æ•°: {MAX_WORKERS}...")
    
    with open(SOLUTION_FILE, 'w', newline='', encoding='utf-8') as f:
        fieldnames = ['BP_ID', 'solution', 'solution_txt_path'] + [f'Image_{i+1}_path' for i in range(12)]
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()

        # ä½¿ç”¨çº¿ç¨‹æ± åŠ é€Ÿ
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_bp = {executor.submit(fetch_bongard_problem, i): i for i in range(START_ID, END_ID + 1)}
            
            for future in future_to_bp:
                result = future.result()
                if result:
                    # å‡†å¤‡ CSV è¡Œæ•°æ®
                    row = {
                        'BP_ID': result['BP_ID'],
                        'solution': result['solution'],
                        'solution_txt_path': result['solution_txt_path']
                    }
                    for j in range(12):
                        row[f'Image_{j+1}_path'] = result['image_paths'][j] if j < len(result['image_paths']) else ''
                    
                    writer.writerow(row)
                    f.flush()  # å®æ—¶åˆ·å…¥ç¡¬ç›˜ï¼Œé˜²æ­¢ä¸¢å¤±

    print(f"\nâœ¨ ä»»åŠ¡å®Œæˆï¼ç»“æœä¿å­˜åœ¨: {SOLUTION_FILE}")